{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementatin\n",
    "\n",
    "#### Coding the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 1024, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_embdgs = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_embdgs = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_embdgs = nn.Dropout(config[\"drop_rate\"])\n",
    "        \n",
    "        # Transformer block placeholder\n",
    "        self.transformer_block = nn.Sequential(*[DummyTransformerBlock(config) for _ in range (config[\"n_layers\"])])\n",
    "\n",
    "\n",
    "        # LayerNorm placeholder\n",
    "        self.normalization_layer = DummyLayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, sequence_len = in_idx.shape\n",
    "        tok_embdgs = self.tok_embdgs(in_idx)\n",
    "        pos_embdgs = self.pos_embdgs(torch.arange(sequence_len, device=in_idx.device))\n",
    "        return self.out_head(self.transformer_block(self.drop_embdgs(tok_embdgs + pos_embdgs)))\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  40, 1107,  761,  284,  467],\n",
      "        [6109, 1110,  318,  257,  649]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "text1 = \"I really need to go\"\n",
    "text2 = \"Every day is a new\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 50257])\n",
      "tensor([[[ 0.7386, -1.2247, -0.3308,  ..., -0.2005,  0.3440, -0.2257],\n",
      "         [-1.9745,  0.9537, -0.6724,  ..., -0.9423, -0.0838,  0.9865],\n",
      "         [-0.8479,  1.4844,  0.0894,  ..., -0.0867,  0.6976, -0.7501],\n",
      "         [-1.2166,  1.5133,  0.2305,  ...,  2.3118, -0.0691,  0.5523],\n",
      "         [-0.4713, -0.8240,  1.2236,  ...,  0.3428, -0.0308, -1.6088]],\n",
      "\n",
      "        [[-1.4299,  0.1282, -0.9106,  ..., -1.6346, -0.3399, -0.5688],\n",
      "         [-0.3886,  0.2121, -0.4795,  ...,  0.0446,  0.2682,  1.3582],\n",
      "         [ 0.6756, -0.5993, -0.4150,  ...,  0.3043,  0.1444, -0.1647],\n",
      "         [ 0.3197, -0.7921, -0.2955,  ...,  1.8263,  0.0524, -0.0759],\n",
      "         [-0.5478, -0.7816,  0.1229,  ..., -0.8491, -1.2927, -1.1232]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPT(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing activations with layer normalization\n",
    "\n",
    "We implement layer normalization to improve stability of the neural network. We've to adjust the output (activation) of each layer such that the mean is 0 and the variance is 1 (unit variance). We apply layer normalization after before and after the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
