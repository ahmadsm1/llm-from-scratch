{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementatin\n",
    "\n",
    "#### Coding the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 1024, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_embdgs = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_embdgs = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_embdgs = nn.Dropout(config[\"drop_rate\"])\n",
    "        \n",
    "        # Transformer block placeholder\n",
    "        self.transformer_block = nn.Sequential(*[DummyTransformerBlock(config) for _ in range (config[\"n_layers\"])])\n",
    "\n",
    "\n",
    "        # LayerNorm placeholder\n",
    "        self.normalization_layer = DummyLayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, sequence_len = in_idx.shape\n",
    "        tok_embdgs = self.tok_embdgs(in_idx)\n",
    "        pos_embdgs = self.pos_embdgs(torch.arange(sequence_len, device=in_idx.device))\n",
    "        return self.out_head(self.transformer_block(self.drop_embdgs(tok_embdgs + pos_embdgs)))\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  40, 1107,  761,  284,  467],\n",
      "        [6109, 1110,  318,  257,  649]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "text1 = \"I really need to go\"\n",
    "text2 = \"Every day is a new\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 50257])\n",
      "tensor([[[ 0.7386, -1.2247, -0.3308,  ..., -0.2005,  0.3440, -0.2257],\n",
      "         [-1.9745,  0.9537, -0.6724,  ..., -0.9423, -0.0838,  0.9865],\n",
      "         [-0.8479,  1.4844,  0.0894,  ..., -0.0867,  0.6976, -0.7501],\n",
      "         [-1.2166,  1.5133,  0.2305,  ...,  2.3118, -0.0691,  0.5523],\n",
      "         [-0.4713, -0.8240,  1.2236,  ...,  0.3428, -0.0308, -1.6088]],\n",
      "\n",
      "        [[-1.4299,  0.1282, -0.9106,  ..., -1.6346, -0.3399, -0.5688],\n",
      "         [-0.3886,  0.2121, -0.4795,  ...,  0.0446,  0.2682,  1.3582],\n",
      "         [ 0.6756, -0.5993, -0.4150,  ...,  0.3043,  0.1444, -0.1647],\n",
      "         [ 0.3197, -0.7921, -0.2955,  ...,  1.8263,  0.0524, -0.0759],\n",
      "         [-0.5478, -0.7816,  0.1229,  ..., -0.8491, -1.2927, -1.1232]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPT(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing activations with layer normalization\n",
    "\n",
    "We implement layer normalization to improve stability of the neural network. We've to adjust the output (activation) of each layer such that the mean is 0 and the variance is 1 (unit variance). We apply layer normalization after before and after the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "example = torch.randn(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "# layer is a Linear layer followed by a non-linear activation layer ReLU (turns -ive vals into 0)\n",
    "\n",
    "result = layer(example)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the mean and variance\n",
    "\n",
    "mean = result.mean(dim=-1, keepdim=True)\n",
    "variance =result.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance:\", variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keepdim=True` ensures the output has the same shape as the input\n",
    "\n",
    "`dim=-1` calculates the statistic at the last dimension, which is the column in a 2D tensor \n",
    "\n",
    "Now we'll apply the normalization layer to the layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized output: tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Normalized Mean: tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Normalized variance: tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "normalized_result = (result - mean) / torch.sqrt(variance)\n",
    "mean = normalized_result.mean(dim=-1, keepdim=True)\n",
    "variance = normalized_result.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized output:\", normalized_result)\n",
    "print(\"Normalized Mean:\", mean)\n",
    "print(\"Normalized variance:\", variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, result):\n",
    "        mean = result.mean(dim=-1, keepdim=True)\n",
    "        variance = result.var(dim=-1, keepdim=True, unbiased = False)\n",
    "        normalized_result = (result-mean) / torch.sqrt(variance+self.eps)\n",
    "        return self.scale*normalized_result + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.eps` prevents division by zero from occuring\n",
    "\n",
    "`unbiased=False` because when calculating the variance, we divide the variance formula by *n*. In Bessel's calculation, we divide it by *n-1* to adjust for bias in sample variance estimation, which results in a biased estimate. In LLM where the embedding size is very large, there's pretty much no difference between *n* and *n-1*. So we choose this approach to mimic GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "variance = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance:\", variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
